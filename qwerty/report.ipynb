# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w-ve-EaVnvfRdolqBxglXt8lZOWaNVX8

Этап №1

Создание модели на необработанных и не заполненных данных

Обработка данных (неполная)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files
uploaded = files.upload()

df = pd.read_csv(next(iter(uploaded)))
display(df.head(10))

df.info()

df.describe()

df.shape

df.info()

object_columns = df.select_dtypes(include=['object'])
object_columns

sns.heatmap(object_columns.isnull(), cmap = "prism")

"""Удаление

Удаление всех колонок, в которых значения отстутствуют на 90+ процентов
"""

procentage = (df.isnull().sum() / len(df)*100).astype(int)
sort = procentage.sort_values(ascending=False)
columns_with100_missing = sort[sort > 90]
columns_list = columns_with100_missing.index.to_list()
columns_list

df.drop(columns_list, axis=1, inplace=True)

object_columns

df.info()

!pip install catboost

"""Поиск дубликатов

Удаление дублирующихся колонок. Но с одной проблемой: значения могут отличаться всего на одну строку, а в остальном дублировать другие. Этот момент нуждается в обработке
"""

def del_duble(df):
  unique_columns = []
  for column in df.columns:
    if not any(df[column].equals(df[col]) for col in unique_columns):
      unique_columns.append(column)
  return df[unique_columns]
df = del_duble(df)
df.head(10)

number_of_duplicates_bylines = df.duplicated().sum()
print(f'Количество дубликатов по строкам {number_of_duplicates_bylines}')

"""в данных очень много дерьма.

Работа с object колонками
"""

object_columns = df.select_dtypes(include=['object'])
for column in object_columns.columns:
    unique_values = object_columns[column].unique()
    print(f"Уникальные значения по столбцу '{column}':")
    print(unique_values)
    print("-" * 50)

"""Поиск и удаление всех колонок с ссылками"""

object_columns = df.select_dtypes(include=['object'])
columns_to_drop = []
for column in object_columns.columns:
    if object_columns[column].astype(str).str.contains('http', na=False).any():
        columns_to_drop.append(column)
df = df.drop(columns=columns_to_drop, axis=1)
print("Удаленные столбцы:", columns_to_drop)

"""Удаление хэша"""

import re
object_col = df.select_dtypes(include=['object'])
hash_pattern = r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$'
columns_to_drop = []
for column in object_col.columns:
    if object_col[column].astype(str).str.match(hash_pattern, na=False).any():
        columns_to_drop.append(column)
df = df.drop(columns=columns_to_drop, axis=1)
print("Удаленные столбцы:", columns_to_drop)

object_col = df.select_dtypes(include=['object'])
for column in object_columns.columns:
    unique_values = object_columns[column].unique()
    print(f"Уникальные значения в столбце '{column}':")
    print(unique_values)
    print("-" * 50)

df.info()

"""Удаление всех остальных мусорных колонок"""

words_list = ['scroll', 'project']
object_columns = df.select_dtypes(include=['object'])
columns_to_drop = []
for column in object_columns.columns:
    if object_columns[column].astype(str).str.contains('|'.join(words_list), na=False).any():
        columns_to_drop.append(column)
df = df.drop(columns=columns_to_drop, axis=1)
print("Удаленные столбцы:", columns_to_drop)

object_columns = df.select_dtypes(include=['object'])
for column in object_columns.columns:
    unique_values = object_columns[column].unique()
    print(f"Уникальные значения по столбцу '{column}':")
    print(unique_values)
    print("-" * 50)

object_columns

"""Положительно коррелирующие признаки"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
# Чтение данных из загруженного файла
df = pd.read_csv(next(iter(uploaded)))
# Обработка данных
object_columns = df.select_dtypes(include=['object'])
columns_to_drop = object_columns.columns.tolist()  # Сохраняем названия столбцов объектного типа
df = df.drop(columns=columns_to_drop, axis=1)  # Удаляем столбцы объектного типа
print("Удаленные столбцы:", columns_to_drop)
# Построение гистограммы
sns.histplot(df.select_dtypes(include=[float, int]).values.flatten(), kde=True)
plt.title("Нормальное распределение")
plt.show()

sample_size = 1000
data = df
# Тест Шапиро-Уилка
w, p_value_sw = stats.shapiro(data)
print(f"Shapiro-Wilk тест: статистика={w}, p-значение={p_value_sw}")
# Тест Колмогорова-Смирнова
d, p_value_ks = stats.kstest(data, 'norm')
print(f"Kolmogorov-Smirnov тест: статистика={d}, p-значение={p_value_ks}")
# Интерпретация результатов
if p_value_sw > 0.05 and p_value_ks > 0.05:
    print("Выборка имеет нормальное распределение")
else:
    print("Выборка не имеет нормальное распределение")

!pip install catboost

"""Base-line модель

---

Работает даже на пропусках, кодирует категориальные колонки
"""

from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
# Определяем категориальные признаки
cat_features = df.select_dtypes(include=['object', 'float64', 'int64']).columns.tolist()
# Удаляем целевой столбец 'target' из списка категориальных признаков
cat_features = [feature for feature in cat_features if feature != 'target']
# Обрабатываем пропущенные значения и преобразуем категориальные признаки в строки
df[cat_features] = df[cat_features].fillna('missing').astype(str)
# Разделяем данные на признаки и целевую переменную
X = df.drop(columns=['target'])  # Предполагаем, что 'target' - целевая переменная
y = df['target']
# Разделение данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42)
# Создаем модель CatBoost
model = CatBoostClassifier(
    iterations=100,
    learning_rate=0.1,
    depth=6,
    cat_features=cat_features,
    eval_metric='AUC')
# Обучение модели
model.fit(
    X_train, y_train,
    eval_set=(X_test, y_test),
    verbose=10)
# Предсказания и оценка модели
y_pred_proba = model.predict_proba(X_test)[:, 1]
roc_auc = roc_auc_score(y_test, y_pred_proba)

# Вывод результатов
print(f"ROC-AUC на тестовом датасете: {roc_auc}")
print(f"Лучшая итерация: {model.best_iteration_}")
print(f"Лучшее значение метрики: {model.best_score_}")

"""ROC-CURV"""

from sklearn.metrics import roc_curve
# Преобразуем y_test в числовой формат
y_test_numeric = y_test.astype(int)
# Построение ROC-кривой
fpr, tpr, thresholds = roc_curve(y_test_numeric, y_pred_proba)
# Построение графика
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid()
plt.show()

"""Этап №2
Работа с пропусками

Заполнение с помощью сингулярного разложения(SVD)

"""

num_cols = df.select_dtypes(include=['float', 'int'])

num_cols

num_cols.info()

import numpy as np
import pandas as pd

def fill_missing_with_mean(X, mask):

    for col in range(X.shape[1]):
        if mask[:, col].sum() > 0:
            col_mean = np.nanmean(X[:, col])
            X[mask[:, col], col] = col_mean
    return X

def round_to_nearest_unique(X, mask):

    for col in range(X.shape[1]):
        if mask[:, col].sum() > 0:
            unique_values = np.unique(X[~mask[:, col], col])
            X[mask[:, col], col] = [unique_values[np.argmin(np.abs(unique_values - x))] for x in X[mask[:, col], col]]
    return X

def add_missing_indicator(X, mask):

    binary_cols = mask.astype(int)
    return np.hstack((X, binary_cols))

def svd_imputer(data, rank=None, max_iter=10, tol=1e-3, round_nearest=True, add_binary=False):


    if isinstance(data, pd.DataFrame):
        X = data.values
    else:
        X = data

    mask = np.isnan(X)


    X = fill_missing_with_mean(X, mask)

    for _ in range(max_iter):

        U, s, Vt = np.linalg.svd(X, full_matrices=False)


        if rank:
            s[rank:] = 0

        new_X = U @ np.diag(s) @ Vt


        diff = np.abs((new_X[mask] - X[mask]) / (X[mask] + 1e-10)).mean()
        if diff < tol:
            break


        X[mask] = new_X[mask]


    if round_nearest:
        X = round_to_nearest_unique(X, mask)


    if add_binary:
        X = add_missing_indicator(X, mask)

    return X


X_imputed = svd_imputer(num_cols, rank=2, max_iter=10, tol=1e-3, round_nearest=True, add_binary=False)


num_cols_imputed = pd.DataFrame(X_imputed, columns=num_cols.columns)

print("Исходный датафрейм:")
print(num_cols)
print("\nДатафрейм после заполнения пропусков:")
print(num_cols_imputed)

sns.heatmap(num_cols_imputed.isnull(), cmap = "prism")

num_cols_imputed.head()

num_cols_imputed['col492'].describe()

num_cols_imputed.info()

object_columns

type(object_columns)

!pip install tensorflow

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam

# Проверка на наличие GPU
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name:
    print(f"Используется GPU: {device_name}")
else:
    print("GPU не обнаружено. Обучение будет медленным на CPU.")

# Убедимся, что в данных нет смешанных типов
object_columns = object_columns.astype(str)  # Преобразуем все данные в строки для однородности

# Кодирование данных
label_encoders = {}
for col in object_columns.columns:
    le = LabelEncoder()
    object_columns[col] = le.fit_transform(object_columns[col].fillna('missing'))
    label_encoders[col] = le

# Определяем размер входных данных
input_dim = object_columns.shape[1]

# Архитектура автоэнкодера
input_layer = Input(shape=(input_dim,))
encoded = Dense(16, activation='relu')(input_layer)  # Уменьшено количество нейронов для ускорения
decoded = Dense(input_dim, activation='linear')(encoded)
autoencoder = Model(input_layer, decoded)
autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

# Обучение модели
batch_size = 32  # Увеличен размер батча
epochs = 10  # Уменьшено количество эпох для тестирования
autoencoder.fit(object_columns, object_columns, epochs=epochs, batch_size=batch_size, verbose=1)

# Получение сжатых данных
df_object = autoencoder.predict(object_columns)
df_object = pd.DataFrame(df_object, columns=object_columns.columns)

# Ограничиваем предсказания диапазоном закодированных значений
for col in object_columns.columns:
    le = label_encoders[col]
    max_label = len(le.classes_) - 1  # Максимальная метка
    df_object[col] = np.clip(df_object[col].round().astype(int), 0, max_label)
    df_object[col] = le.inverse_transform(df_object[col])

print(df_object)

for column in object_columns:
    if column in df.columns:  # Проверка наличия столбца в датафрейме
        if df[column].isnull().all():
            # Если весь столбец состоит только из NaN
            df[column] = df[column].fillna('missing')
        else:
            # Заполняем пропуски модой
            mode_value = df[column].mode().iloc[0]  # Безопасное обращение к моде
            df[column] = df[column].fillna(mode_value)
    else:
        print(f"Столбец {column} отсутствует в датафрейме.")

missing_columns = [col for col in object_columns if col not in df.columns]
if missing_columns:
    print(f"Отсутствующие столбцы: {missing_columns}")

sns.heatmap(object_columns.isnull(), cmap='prism')

full_df = pd.concat([object_columns, num_cols_imputed], axis=1)
full_df.head()

full_df.info()

sns.heatmap(full_df.isnull(), cmap='prism')

full_df['col600']

full_df['report_date']

# Добавление искусственного положительного класса
positive_samples = full_df.sample(n=10, random_state=42).copy()
positive_samples['target'] = 1
full_df = pd.concat([full_df, positive_samples], ignore_index=True)
# Проверка нового распределения классов
print("Распределение классов после генерации:")
print(full_df['target'].value_counts())

import pandas as pd
import numpy as np

# Удаление пропусков в 'target'
cleaned_df = full_df.dropna(subset=['target'])

# Проверка распределения классов
print("Размер данных после удаления пропусков:", cleaned_df.shape)
print("Распределение классов в 'target':")
print(cleaned_df['target'].value_counts())

# Проверка, есть ли положительный класс
if 1 not in cleaned_df['target'].unique():
    print("Данные не содержат положительного класса.")
    # TODO: добавить положительный класс или пересмотреть данные

import pandas as pd
import matplotlib.pyplot as plt

# Проверка на наличие пропусков
if full_df['target'].isnull().any():
    print("Пропуски в целевой переменной обнаружены.")
    print(full_df['target'].isnull().sum(), "пропусков")
else:
    print("Пропусков в целевой переменной нет.")

# Проверка уникальных значений в 'target'
unique_values = full_df['target'].unique()
print("Уникальные значения в 'target':", unique_values)

# Проверка на баланс классов (если целевой признак бинарный или категориальный)
if len(unique_values) <= 10:  # Если категорий немного, рисуем график
    target_counts = full_df['target'].value_counts()
    print("\nРаспределение классов:")
    print(target_counts)

    # Визуализация распределения классов
    target_counts.plot(kind='bar', color='skyblue', edgecolor='black')
    plt.title('Распределение классов в целевой переменной')
    plt.xlabel('Классы')
    plt.ylabel('Количество')
    plt.show()
else:
    print("\nРаспределение классов сложно визуализировать из-за большого количества уникальных значений.")

# Проверка на возможные ошибки в значениях (например, отрицательные значения, если их быть не должно)
if full_df['target'].dtype in ['int64', 'float64']:  # Если 'target' числовой
    if (full_df['target'] < 0).any():
        print("\nОбнаружены отрицательные значения в 'target'. Проверьте данные.")
    else:
        print("\nЦелевая переменная не содержит отрицательных значений.")

# Если 'target' числовой, проверка статистик
if full_df['target'].dtype in ['int64', 'float64']:
    print("\nСтатистики целевой переменной:")
    print(full_df['target'].describe())

correlation_matrix = full_df.corr()
target_correlation = correlation_matrix['target']
positive_correlation_columns = target_correlation[abs(target_correlation) >= 0.09].index.tolist()
print("Столбцы имеющие корреляцию:", positive_correlation_columns)

"""111 основных признаков, которые имеют с целевой переменной как отрицательную, так и положительную корреляцию"""

len(positive_correlation_columns)

type(positive_correlation_columns)

"""колонки 2663 отсутствует (обозначает вероятность наличия у клиента авто)"""

important_columns = full_df[positive_correlation_columns]
important_columns.describe()

correlation_matrix = full_df.corr()
target_correlation = correlation_matrix['target']
probability_corr = target_correlation['col480']
probability_corr

plt.figure(figsize=(25,20))
sns.heatmap(important_columns.corr().round(2), cmap='crest', annot=True)

important_columns = important_columns.drop(['target'], axis=1)

important_columns['col472']

important_columns.corr()

def merge_high_correlation(df, threshold=0.9):
    correlation_matrix = df.corr()
    high_corr_pairs = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i + 1, len(correlation_matrix.columns)):
            if abs(correlation_matrix.iloc[i, j]) >= threshold:
                high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j]))
    for col1, col2 in high_corr_pairs:
        if col1 in df.columns and col2 in df.columns:
            df[col1] = df[[col1, col2]].mean(axis=1)

            df = df.drop(columns=[col2])
    return df
df_merged = merge_high_correlation(important_columns, threshold=0.75)
print("Датафрейм после объединения избыточных колонок:")
print(df_merged)

df_merged.head()

df_merged.shape