# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sNKMz-GLZ6NGAcYJ0MxchBixjaCbqe3k
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""Загрузка датасета"""

from google.colab import files
uploaded = files.upload()

"""Анализ данных"""

df = pd.read_csv(next(iter(uploaded)))
display(df.head(10))

df.info()

"""Анализ пропущенных данных"""

sns.heatmap(df.isnull(), cmap = "prism")

"""Удаление колонок с 100% пропущенными данными"""

# Рассчитываем процент отсутствующих значений для каждого столбца
missing_percentage = df.isnull().mean() * 100
# Получаем список столбцов с 100% отсутствующими значениями
columns_to_drop = missing_percentage[missing_percentage == 100].index.tolist()
# Удаляем эти столбцы из DataFrame
df.drop(columns=columns_to_drop, axis=1, inplace=True)

"""Заполнение пропусков"""

numerical_columns = df.select_dtypes(include=['number']).columns
categorical_columns = df.select_dtypes(include=['object']).columns

"""Для числовых данных: заполняем медианой"""

from sklearn.impute import SimpleImputer
# Создаем объект SimpleImputer для замены пропущенных значений медианой
num_imputer = SimpleImputer(strategy="median")
# Применяем его к числовым столбцам DataFrame
df[numerical_columns] = num_imputer.fit_transform(df[numerical_columns])

"""Для категориальных данных: заполняем наиболее частым значением"""

cat_imputer = SimpleImputer(strategy="most_frequent")
df[categorical_columns] = cat_imputer.fit_transform(df[categorical_columns])

"""Проверка после обработки"""

print("\nПроверка на пропущенные значения:")
print(df.isnull().sum().sort_values(ascending=False))

"""Визуализация пропущенных данных (должна быть пустая тепловая карта)"""

plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cmap="prism", cbar=False)
plt.title("Пропущенные значения после обработки")
plt.show()

"""Основные статистики данных"""

df.info()

df.describe()

df.head()

"""колонка col1454 содержит хэш клиентов, получать информацию от туда не предоставляется возможным, принято решение её удалить"""

df.drop(['col1454'], axis=1, inplace=True)

df.head()

"""Поскольку вручную анализировать 80 колонок слишком сложно, целесообразно использовать готовые языковые модели.
Однако, чтобы оптимизировать процесс, сначала можно перекодировать значения и выполнить предварительный анализ.
Это поможет выявить наиболее значимые признаки, которые влияют на целевую переменную.

Проверка на дубликаты
"""

number_of_duplicates_bylines = df.duplicated().sum()
print(f'Количество дубликатов по строкам {number_of_duplicates_bylines}')
number_of_duplicates_bycolumns = df.T.duplicated().sum()
print(f'Количество дубликатов по колонкам {number_of_duplicates_bycolumns}')

def del_dups(df):
    columns_to_drop = []
    for i, column1 in enumerate(df.columns):
        for column2 in df.columns[i+1:]:
            if df[column1].equals(df[column2]):
                columns_to_drop.append(column2)
    return df.drop(columns=columns_to_drop)
del_dups(df)
df.head(10)

number_of_duplicates_bycolumns = df.T.duplicated().sum()
print(f'Количество дубликатов по колонкам {number_of_duplicates_bycolumns}')

df.shape

"""Гистограмма с графиком плотности"""

data = np.random.normal(loc=0, scale=1, size=1000)
sns.histplot(data, kde=True)
plt.title("Гистограмма и KDE нормального распределения")
plt.show()

"""Создание выборки с нормальным распределением"""

from scipy import stats
# Создание выборки с нормальным распределением
data = np.random.normal(loc=0, scale=1, size=1000)
# Тест Шапиро-Уилка
w, p_value = stats.shapiro(data)
print(f"Shapiro-Wilk тест: статистика={w}, p-значение={p_value}")
# Тест Колмогорова-Смирнова
d, p_value = stats.kstest(data, 'norm')
print(f"Kolmogorov-Smirnov тест: статистика={d}, p-значение={p_value}")

"""Нормальное распределение и гистограмма(создание выборки)"""

sample_size = 1000
data = np.random.normal(loc=0, scale=1, size=sample_size)
# Визуализация выборки
sns.histplot(data, kde=True)
plt.title("Гистограмма и KDE нормального распределения")
plt.show()
# Тест Шапиро-Уилка
w, p_value_sw = stats.shapiro(data)
print(f"Shapiro-Wilk тест: статистика={w}, p-значение={p_value_sw}")
# Тест Колмогорова-Смирнова
d, p_value_ks = stats.kstest(data, 'norm')
print(f"Kolmogorov-Smirnov тест: статистика={d}, p-значение={p_value_ks}")
# Интерпретация результатов
if p_value_sw > 0.05 and p_value_ks > 0.05:
    print("Выборка имеет нормальное распределение")
else:
    print("Выборка не имеет нормальное распределение")

"""Датасет имеет нормальное распределение, что упрощает применение статистических методов анализа и позволяет использовать предпосылки классических моделей машинного обучения"""

df.describe()

# Создание выборки с пуассоновским распределением
data_pois = np.random.poisson(lam=3, size=1000)
# Визуализация
sns.histplot(data_pois, kde=False)
plt.title("Пуассоновское распределение")
plt.show()

df.info()

object_columns = df.select_dtypes(include=['object'])
object_columns

"""Обучение модели Random Forest"""

from sklearn.preprocessing import OrdinalEncoder
from sklearn.ensemble import RandomForestClassifier
#преобразуем категориальные данные
categorical_columns = df.select_dtypes(include=['object']).columns
if len(categorical_columns) > 0:
    print(f"Категориальные столбцы: {categorical_columns.tolist()}")
    ordinal_encoder = OrdinalEncoder()
    df[categorical_columns] = ordinal_encoder.fit_transform(df[categorical_columns])
    # Преобразуем дату в числовой формат, если есть столбцы с датами
date_columns = df.select_dtypes(include=['datetime64[ns]', 'object']).columns
for column in date_columns:
    if pd.to_datetime(df[column], errors='coerce').notna().all():
        df[column] = pd.to_datetime(df[column]).map(lambda x: x.toordinal())

from sklearn.preprocessing import OrdinalEncoder
categorical_columns = df.select_dtypes(include=['object']).columns.tolist()
ordinal_encoder = OrdinalEncoder()
encoded_data = ordinal_encoder.fit_transform(df[categorical_columns])
df[categorical_columns] = encoded_data.astype(int)
df

"""Убедимся, что больше нет текстовых данных"""

print("Типы данных после преобразования:")
print(df.dtypes)

# Обучение модели RandomForestClassifier
X = df.drop(columns=['client_id', 'target'], errors='ignore')
y = df['target']
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X, y)

"""Получение важности признаков"""

if 'target' in df.columns:
    X = df.drop(columns=['client_id', 'target'], errors='ignore')
    y = df['target']
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X, y)
    feature_importances = pd.Series(model.feature_importances_, index=X.columns)
    feature_importances.sort_values(ascending=False, inplace=True)

"""Визуализация важности признаков"""

import matplotlib.pyplot as plt
feature_importances.plot(kind='bar', color='skyblue')
# Показать график
plt.show()

# Построение корреляционной матрицы
def plot_correlation_matrix(data):
#Функция построения корреляционной матрицы.
    plt.figure(figsize=(25, 20))
    corr_matrix = data.corr().round(2)  # Вычисление корреляции
    sns.heatmap(corr_matrix, cmap="crest", annot=True)
    plt.title("Корреляционная матрица после обработки данных")
    plt.show()
# Построение корреляционной матрицы
print("Построение корреляционной матрицы после обработки данных...")
plot_correlation_matrix(df)

"""Наиболее влияющие признаки можно определить с помощью корреляционного анализа или методов оценки важности признаков, таких как feature importance из модели случайного леса или SHAP values"""

# Рассчитываем корреляцию с целевой переменной
corr_matrix = df.corr()['target'].round(2)
corr_matrix = corr_matrix.fillna(0)
# Сортируем по убыванию
sort = corr_matrix.sort_values(ascending=False)
# Выводим результаты
print(sort)

"""нормализацию числовых данных в таблице (DataFrame) с помощью класса StandardScaler из библиотеки scikit-learn"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

"""Выполняем нормализацию данных в диапазон [0,1] с использованием MinMaxScaler из библиотеки scikit-learn, а также импортируем инструмент для заполнения пропущенных значений, KNNImputer."""

from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import KNNImputer
scaler = MinMaxScaler()
df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

"""Вычисляем корреляцию между всеми столбцами DataFrame df и целевым столбцом 'target', затем все результаты сортируются в порядке убывания"""

import pandas as pd
# Вычисляем корреляцию с целевой переменной
corr_matrix = df.corr()['target'].dropna().round(2)
# Сортируем значения по убыванию
sorted_corr = corr_matrix.sort_values(ascending=False)
# Преобразуем в DataFrame для отображения в табличной форме
corr_table = pd.DataFrame(sorted_corr).reset_index()
corr_table.columns = ['Feature', 'Correlation with target']
# Выводим таблицу
print(corr_table)

"""Добавляем код определяющий столбцы, содержащие выбросы, в таблице (DataFrame) df на основе метода межквартильного размаха (IQR)"""

outlier_cols = []
for column in df.columns:
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    outliers = (df[column] < Q1 - 1.5 * IQR) | (df[column] > Q3 + 1.5 * IQR)
    if any(outliers):
        outlier_cols.append(column)
print("Колонки с выбросами:", outlier_cols)
len(outlier_cols) #Команда len(outlier_cols) возвращает количество элементов в списке outlier_cols

len(outlier_cols)

"""Анализ выбросов"""

for column in outlier_cols:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=df[column])
    plt.title(f"Box Plot для колонки {column}")
    plt.show()

"""Определение и удаление выбросов"""

from os import defpath
def detect_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers
# функцияя для удаления выбросов
def remove_outliers(df, column):
    outliers = detect_outliers_iqr(df, column)
    df = df.drop(outliers.index)
    return df
# Визуализация выбросов
def visualize_outliers(df, column):
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=df[column], color='skyblue')
    plt.title(f'Box Plot for {column}')
    plt.show()
# Определение и удаление выбросов
for column in df:
    df = remove_outliers(df, column)
    visualize_outliers(df, column)
# Вывод обработанных данных
print("Обработанные данные:")
print(df)

"""Код обновления DataFrame df, оставляет только строки с определёнными индексами, применяется в контексте удаления выбросов"""

remaining_indices = df.index
df = df.loc[remaining_indices]

for column in outlier_cols:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=df[column])
    plt.title(f"Box Plot для колонки {column}")
    plt.show()

"""Строим тепловую карту корреляций для всех числовых колонок df с использованием библиотеки Seaborn"""

plt.figure(figsize = (25,20))
sns.heatmap(df.corr().round(2), cmap="crest", annot=True)

"""Строим тепловую карту корреляции всех признаков с целевой переменной"""

cor_matrix = df.corr()
target_corr = cor_matrix['target']
plt.figure(figsize=(8, 2))  # Уменьшаем высоту для лучшего отображения
sns.heatmap(target_corr.to_frame(), annot=True, cmap="crest")
plt.title("Корреляция признаков с целевой переменной")
plt.show()

df.info()

# Удаляем дубликаты по колонкам (то есть, по строкам, а не по индексам)
df_no_duplicates = df.loc[:, ~df.T.duplicated()].copy()
#Теперь выводим количество дубликатов по колонкам
number_of_duplicates_bycolumns = df_no_duplicates.T.duplicated().sum()
print(f'Количество дубликатов по колонкам: {number_of_duplicates_bycolumns}')

df.shape

corr_matrix = df.corr()['target'].dropna().round(2)
sort = corr_matrix.sort_values(ascending=False)
print(sort)

"""заполнение пропусков с помощью KNN"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import KNNImputer
# Нормализация данных в диапазон [0, 1]
scaler = MinMaxScaler()
df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

# Заполнение пропусков с использованием KNNImputer
imputer = KNNImputer(n_neighbors=3)
df_imputed = pd.DataFrame(imputer.fit_transform(df_normalized), columns=df.columns)

# Обратное преобразование в исходный масштаб (если нужно)
df_original_scale = pd.DataFrame(scaler.inverse_transform(df_imputed), columns=df.columns)

sns.heatmap(df.isnull(), cmap = "crest")

df.head()

df.isnull().sum().sort_values(ascending=False)

df.rename(columns = {'col2663': 'вероятность'}, inplace=True)

df.head()

plt.hist(df, bins=10, edgecolor='black')
plt.title("Гистограмма распределения данных")
plt.xlabel("Значения")
plt.ylabel("Частота")
plt.show()

"""Оценка точности модели с использованием ROC-кривой (Receiver Operating Characteristic curve).
Можно воспользоваться библиотеками matplotlib и sklearn
"""

import matplotlib.pyplot as plt
import numpy as np
# Получаем предсказания
y_pred = model.predict(X_test)
# Строим кривую для сравнения истинных и предсказанных значений
plt.figure(figsize=(8, 6))
# Сортируем для построения плавной линии
sorted_indices = np.argsort(y_test)
plt.plot(np.array(y_test)[sorted_indices], np.array(y_pred)[sorted_indices], color='darkorange', lw=2)
# Строим диагональ (линия идеального прогноза)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='navy', lw=2, linestyle='--')
# Добавляем метки и заголовок
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.title('Regression Curve')
plt.show()